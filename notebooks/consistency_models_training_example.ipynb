{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistency Models Training Example\n",
    "\n",
    "[![arXiv](https://img.shields.io/badge/arXiv-2301.01469-<COLOR>.svg)](https://arxiv.org/abs/2303.01469) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Kinyugo/consistency_models/blob/main/notebooks/consistency_models_training_example.ipynb) [![GitHub Repo stars](https://img.shields.io/github/stars/Kinyugo/consistency_models?style=social) ](https://github.com/Kinyugo/consistency_models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📖 Introduction\n",
    "\n",
    "Consistency Models are a new family of generative models that achieve high sample quality without adversarial training. They support fast one-step generation by design, while still allowing for few-step sampling to trade compute for sample quality. They also support zero-shot data editing, like image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Setup\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q lightning diffusers transformers gdown torchmetrics lpips --no-cache --upgrade\n",
    "%pip install -q -e git+https://github.com/Kinyugo/consistency_models.git#egg=consistency_models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading and Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1FnzQLDPs-IlTTEr14YyENKjTYqZfn8mS && tar -xf butterflies256.tar.gz # Butterflies Dataset\n",
    "# !gdown 1m1QrNnKJy7hEzUQusyD3th_La775QKUV && tar -xf abstract_art.tar.gz  # Abstract Art Dataset\n",
    "# !gdown 1VJow74U3H7KG_HOiP1WWo6LoqoE3azJj && tar -xf anime_faces.tar.gz # Anime Faces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "from lightning.pytorch import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "class ImageDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        transform: Callable = None,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 2,\n",
    "        pin_memory: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        self.dataset = ImageFolder(self.data_dir, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "def transform_fn(image_size: Tuple[int, int]) -> T.Compose:\n",
    "    return T.Compose(\n",
    "        [\n",
    "            T.Resize(image_size),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Lambda(lambda x: (x * 2) - 1),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, image_size: Tuple[int, int]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_fn = UNet2DModel(\n",
    "            sample_size=image_size,\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            layers_per_block=2,\n",
    "            block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "            down_block_types=(\n",
    "                \"DownBlock2D\",\n",
    "                \"DownBlock2D\",\n",
    "                \"DownBlock2D\",\n",
    "                \"DownBlock2D\",\n",
    "                \"AttnDownBlock2D\",\n",
    "                \"DownBlock2D\",\n",
    "            ),\n",
    "            up_block_types=(\n",
    "                \"UpBlock2D\",\n",
    "                \"AttnUpBlock2D\",\n",
    "                \"UpBlock2D\",\n",
    "                \"UpBlock2D\",\n",
    "                \"UpBlock2D\",\n",
    "                \"UpBlock2D\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model_fn(*args, **kwargs, return_dict=True).sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import Tensor\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "def plot_distribution(x: Tensor, title: str) -> plt.Figure:\n",
    "    x = x.detach().cpu()\n",
    "    batch_size = int(x.shape[0])\n",
    "\n",
    "    fig, axes = plt.subplots(2, max(batch_size // 2, 1), constrained_layout=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        hist, edges = torch.histogram(x[b], density=True)\n",
    "        axes[b].plot(edges[:-1], hist)\n",
    "\n",
    "    mean, std = x.mean(), x.std()\n",
    "    fig.suptitle(f\"{title} | Mean: {mean:.4f} Std: {std:.4f}\")\n",
    "    fig.supxlabel(\"X\")\n",
    "    fig.supylabel(\"Density\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def log_images(\n",
    "    logger: TensorBoardLogger, images: Tensor, title: str, global_step: int\n",
    ") -> None:\n",
    "    grid = make_grid(images.clamp(-1.0, 1.0), value_range=(-1.0, 1.0), normalize=True)\n",
    "    logger.experiment.add_image(title, grid, global_step)\n",
    "\n",
    "\n",
    "def log_distribution(\n",
    "    logger: TensorBoardLogger, x: Tensor, title: str, global_step: int\n",
    ") -> None:\n",
    "    figure = plot_distribution(x, title)\n",
    "    logger.experiment.add_figure(title, figure, global_step)\n",
    "\n",
    "\n",
    "def log_samples(\n",
    "    logger: TensorBoardLogger,\n",
    "    samples: Tensor,\n",
    "    tag: str,\n",
    "    global_step: int,\n",
    ") -> None:\n",
    "    log_images(logger, samples, f\"images/{tag}\", global_step)\n",
    "    log_distribution(logger, samples, f\"distribution/{tag}\", global_step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "from lightning.pytorch import LightningModule\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "\n",
    "from consistency_models.consistency_models import (\n",
    "    ConsistencySamplingAndEditing,\n",
    "    ConsistencyTraining,\n",
    "    ema_decay_rate_schedule,\n",
    "    karras_schedule,\n",
    "    timesteps_schedule,\n",
    ")\n",
    "from consistency_models.utils import update_ema_model\n",
    "\n",
    "\n",
    "class LitConsistencyModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        consistency_training: ConsistencyTraining,\n",
    "        consistency_sampling: ConsistencySamplingAndEditing,\n",
    "        unet: UNet2DModel,\n",
    "        ema_unet: UNet2DModel,\n",
    "        initial_ema_decay_rate: float = 0.95,\n",
    "        lr: float = 2e-4,\n",
    "        betas: Tuple[float, float] = (0.5, 0.999),\n",
    "        lr_scheduler_start_factor: float = 1 / 3,\n",
    "        lr_scheduler_iters: int = 500,\n",
    "        sample_every_n_steps: int = 500,\n",
    "        num_samples: int = 8,\n",
    "        num_sampling_steps: List[int] = [1, 2, 5],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(\n",
    "            ignore=[\"consistency_training\", \"consistency_sampling\", \"unet\", \"ema_unet\"]\n",
    "        )\n",
    "\n",
    "        self.consistency_training = consistency_training\n",
    "        self.consistency_sampling = consistency_sampling\n",
    "        self.unet = unet\n",
    "        self.ema_unet = ema_unet\n",
    "        self.initial_ema_decay_rate = initial_ema_decay_rate\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.lr_scheduler_start_factor = lr_scheduler_start_factor\n",
    "        self.lr_scheduler_iters = lr_scheduler_iters\n",
    "        self.sample_every_n_steps = sample_every_n_steps\n",
    "        self.num_samples = num_samples\n",
    "        self.num_sampling_steps = num_sampling_steps\n",
    "\n",
    "        self.lpips = LearnedPerceptualImagePatchSimilarity(net_type=\"alex\")\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Union[Tensor, List[Tensor]], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        # Drop labels if present\n",
    "        if isinstance(batch, list):\n",
    "            batch = batch[0]\n",
    "\n",
    "        # Compute predicted and target\n",
    "        predicted, target = self.consistency_training(\n",
    "            self.unet, self.ema_unet, batch, self.global_step, self.trainer.max_steps\n",
    "        )\n",
    "\n",
    "        # Compute losses\n",
    "        clamp = lambda x: x.clamp(min=-1.0, max=1.0)\n",
    "        lpips_loss = self.lpips(clamp(predicted), clamp(target))\n",
    "        mse_loss = F.mse_loss(clamp(predicted), clamp(target))\n",
    "        overflow_loss = F.mse_loss(predicted, clamp(predicted).detach())\n",
    "        loss = lpips_loss + mse_loss, overflow_loss\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"lpips_loss\": lpips_loss,\n",
    "                \"mse_loss\": mse_loss,\n",
    "                \"overflow_loss\": overflow_loss,\n",
    "                \"train_loss\": loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Sample and log samples\n",
    "        if self.global_step % self.sample_every_n_steps == 0:\n",
    "            self.__sample_and_log_samples(batch)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_batch_end(self, *args) -> None:\n",
    "        # Update the ema model\n",
    "        num_timesteps = timesteps_schedule(\n",
    "            self.global_step,\n",
    "            self.trainer.max_steps,\n",
    "            initial_timesteps=self.consistency_training.initial_timesteps,\n",
    "            final_timesteps=self.consistency_training.final_timesteps,\n",
    "        )\n",
    "        ema_decay_rate = ema_decay_rate_schedule(\n",
    "            num_timesteps,\n",
    "            initial_ema_decay_rate=self.initial_ema_decay_rate,\n",
    "            initial_timesteps=self.consistency_training.initial_timesteps,\n",
    "        )\n",
    "        self.ema_unet = update_ema_model(self.ema_unet, self.unet, ema_decay_rate)\n",
    "        self.log_dict(\n",
    "            {\"num_timesteps\": num_timesteps, \"ema_decay_rate\": ema_decay_rate}\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = optim.Adam(self.unet.parameters(), lr=self.lr, betas=self.betas)\n",
    "        sched = optim.lr_scheduler.LinearLR(\n",
    "            opt,\n",
    "            start_factor=self.lr_scheduler_start_factor,\n",
    "            total_iters=self.lr_scheduler_iters,\n",
    "        )\n",
    "        sched = {\"scheduler\": sched, \"interval\": \"step\"}\n",
    "\n",
    "        return [opt], [sched]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __sample_and_log_samples(self, batch: Tensor) -> None:\n",
    "        # Ensure the number of samples does not exceed the batch size\n",
    "        num_samples = min(self.num_samples, batch.shape[0])\n",
    "        noise = torch.randn_like(batch[:num_samples])\n",
    "\n",
    "        # Log ground truth samples\n",
    "        log_samples(\n",
    "            self.logger,\n",
    "            batch[:num_samples],\n",
    "            f\"ground_truth\",\n",
    "            self.global_step,\n",
    "        )\n",
    "\n",
    "        for steps in self.num_sampling_steps:\n",
    "            # Sample an extra step and reverse the schedule as the last step (sigma=sigma_min)\n",
    "            # is useless as the model returns identity\n",
    "            sigmas = karras_schedule(\n",
    "                steps + 1,\n",
    "                sigma_min=self.consistency_training.sigma_min,\n",
    "                sigma_max=self.consistency_training.sigma_max,\n",
    "                rho=self.consistency_training.rho,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "            sigmas = sigmas.flipud()[:-1]\n",
    "\n",
    "            samples = self.consistency_sampling(\n",
    "                self.unet, noise, sigmas, clip_denoised=True, verbose=True\n",
    "            )\n",
    "            samples = samples.clamp(min=-1.0, max=1.0)\n",
    "\n",
    "            # Generated samples\n",
    "            log_samples(\n",
    "                self.logger,\n",
    "                samples,\n",
    "                f\"generated_samples-steps={steps}\",\n",
    "                self.global_step,\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def save_model_ckpt(model: nn.Module, ckpt_path: str) -> None:\n",
    "    os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "\n",
    "def load_model_ckpt(model: nn.Module, ckpt_path: str) -> nn.Module:\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "\n",
    "@dataclass()\n",
    "class Config:\n",
    "    # Reproducibility\n",
    "    seed: int = 0\n",
    "\n",
    "    # Data Config\n",
    "    image_size: Tuple[int, int] = (128, 128)\n",
    "    data_dir: str = \"butterflies256\"\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 2\n",
    "    pin_memory: bool = True\n",
    "\n",
    "    # Consistency Model Config\n",
    "    sigma_min: float = 0.002\n",
    "    sigma_max: float = 80.0\n",
    "    rho: float = 7.0\n",
    "    sigma_data: float = 0.5\n",
    "    initial_timesteps: int = 2\n",
    "    final_timesteps: int = 150\n",
    "    initial_ema_decay_rate: float = 0.95\n",
    "\n",
    "    # Lightning Model Config\n",
    "    lr: float = 2e-5\n",
    "    betas: Tuple[float, float] = (0.5, 0.999)\n",
    "    lr_scheduler_start_factor: float = 1 / 3\n",
    "    lr_scheduler_iters: int = 500\n",
    "    sample_every_n_steps: int = 10_000\n",
    "    num_samples: int = 8\n",
    "    num_sampling_steps: List[int] = field(default_factory=lambda: [1, 2, 5])\n",
    "\n",
    "    # Tensorboard Logger\n",
    "    name: str = \"consistency_models\"\n",
    "    version: str = \"butterflies256_100k\"\n",
    "\n",
    "    # Checkpoint Callback\n",
    "    every_n_train_steps: int = 10_000\n",
    "\n",
    "    # Trainer\n",
    "    accelerator: str = \"auto\"\n",
    "    max_steps: int = 100_001\n",
    "    gradient_clip_val: float = 1.0\n",
    "    log_every_n_steps: int = 20\n",
    "    precision: Union[int, str] = 16\n",
    "    detect_anomaly: bool = False\n",
    "\n",
    "    # Training Loop\n",
    "    skip_training: bool = False\n",
    "\n",
    "    # Model checkpoint\n",
    "    model_ckpt_path: str = \"checkpoints/unet.pt\"\n",
    "    resume_ckpt_path: Optional[str] = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import matplotlib\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "def run_training(config: Config) -> None:\n",
    "    # -------------------------------------------\n",
    "    # Reproducibility\n",
    "    # -------------------------------------------\n",
    "    L.seed_everything(config.seed)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # Configure Matplotlib\n",
    "    # -------------------------------------------\n",
    "    # Prevents pixelated fonts on figures\n",
    "    matplotlib.use(\"webagg\")\n",
    "    matplotlib.style.use([\"ggplot\", \"fast\"])\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # Data & Transforms\n",
    "    # -------------------------------------------\n",
    "    transform = transform_fn(config.image_size)\n",
    "    datamodule = ImageDataModule(\n",
    "        config.data_dir,\n",
    "        transform=transform,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory,\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Models\n",
    "    # ------------------------------------------\n",
    "    consistency_training = ConsistencyTraining(\n",
    "        sigma_min=config.sigma_min,\n",
    "        sigma_max=config.sigma_max,\n",
    "        rho=config.rho,\n",
    "        sigma_data=config.sigma_data,\n",
    "        initial_timesteps=config.initial_timesteps,\n",
    "        final_timesteps=config.final_timesteps,\n",
    "    )\n",
    "    consistency_sampling = ConsistencySamplingAndEditing(\n",
    "        sigma_min=config.sigma_min, sigma_data=config.sigma_data\n",
    "    )\n",
    "    unet = UNet(config.image_size)\n",
    "    ema_unet = UNet(config.image_size)\n",
    "    ema_unet.load_state_dict(unet.state_dict())\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Lit Model\n",
    "    # ------------------------------------------\n",
    "    lit_consistency_model = LitConsistencyModel(\n",
    "        consistency_training,\n",
    "        consistency_sampling,\n",
    "        unet,\n",
    "        ema_unet,\n",
    "        initial_ema_decay_rate=config.initial_ema_decay_rate,\n",
    "        lr=config.lr,\n",
    "        betas=config.betas,\n",
    "        lr_scheduler_start_factor=config.lr_scheduler_start_factor,\n",
    "        lr_scheduler_iters=config.lr_scheduler_iters,\n",
    "        sample_every_n_steps=config.sample_every_n_steps,\n",
    "        num_samples=config.num_samples,\n",
    "        num_sampling_steps=config.num_sampling_steps,\n",
    "    )\n",
    "    # -----------------------------------------\n",
    "    # Trainer\n",
    "    # ------------------------------------------\n",
    "    logger = TensorBoardLogger(name=config.name, version=config.version)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        every_n_train_steps=config.every_n_train_steps\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator=config.accelerator,\n",
    "        max_steps=config.max_steps,\n",
    "        gradient_clip_val=config.gradient_clip_val,\n",
    "        log_every_n_steps=config.log_every_n_steps,\n",
    "        precision=config.precision,\n",
    "        detect_anomaly=config.detect_anomaly,\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Run Training\n",
    "    # ------------------------------------------\n",
    "    if not config.skip_training:\n",
    "        trainer.fit(\n",
    "            lit_consistency_model,\n",
    "            datamodule=datamodule,\n",
    "            ckpt_path=config.resume_ckpt_path,\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # Save Checkpoint\n",
    "    # -------------------------------------------\n",
    "    save_model_ckpt(lit_consistency_model.unet, config.model_ckpt_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "run_training(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎲 Sampling & Zero-shot Editing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot_images(images: Tensor, cols: int = 4) -> None:\n",
    "    rows = max(images.shape[0] // cols, 1)\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(16, 4 * rows))\n",
    "    axs = axs.flatten()\n",
    "    for i, image in enumerate(images):\n",
    "        axs[i].imshow(image.permute(1, 2, 0).numpy() / 2 + 0.5)\n",
    "        axs[i].set_axis_off()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(config.image_size)\n",
    "consistency_sampling_and_editing = ConsistencySamplingAndEditing(\n",
    "    config.sigma_min, config.sigma_data\n",
    ")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = load_model_ckpt(model, \"checkpoints/unet.pth\")\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sample Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ImageDataModule(\n",
    "    config.data_dir,\n",
    "    transform=transform_fn(config.image_size),\n",
    "    batch_size=4,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dm.setup()\n",
    "dm.prepare_data()\n",
    "\n",
    "dl = dm.train_dataloader()\n",
    "batch, _ = next(iter(dl))\n",
    "batch = batch.to(device)\n",
    "\n",
    "plot_images(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    samples = consistency_sampling_and_editing(\n",
    "        model,\n",
    "        torch.randn((4, 3, 128, 128)),\n",
    "        sigmas=[80.0],  # Use more steps for better samples e.g 2-5\n",
    "        clip_denoised=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "plot_images(samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomErasing\n",
    "\n",
    "random_erasing = RandomErasing(p=1.0, scale=(0.2, 0.5), ratio=(0.5, 0.5))\n",
    "masked_batch = random_erasing(batch)\n",
    "mask = torch.logical_not(batch == masked_batch)\n",
    "\n",
    "plot_images(masked_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inpainted_batch = consistency_sampling_and_editing(\n",
    "        model,\n",
    "        masked_batch,\n",
    "        sigmas=[5.23, 2.25],\n",
    "        mask=mask.float(),\n",
    "        clip_denoised=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "plot_images(torch.cat((masked_batch, inpainted_batch), dim=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_a = batch[: batch.shape[0] // 2]\n",
    "batch_b = batch[batch.shape[0] // 2 :]\n",
    "\n",
    "plot_images(torch.cat((batch_a, batch_b), dim=0), cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    interpolated_batch = consistency_sampling_and_editing.interpolate(\n",
    "        model,\n",
    "        batch_a,\n",
    "        batch_b,\n",
    "        ab_ratio=0.5,\n",
    "        sigmas=[5.23, 2.25],\n",
    "        clip_denoised=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "plot_images(interpolated_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c339664639c3e5019e3803d0baff2aab4fdaac0204aae143f6ed0f1a6cb76161"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
